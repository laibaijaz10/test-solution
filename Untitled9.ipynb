{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Section C:[Practical â€“ Development- Programming Part] qno1***"
      ],
      "metadata": {
        "id": "KDOxz1pIbe7c"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "url = \"/content/heart.csv\"\n",
        "df = pd.read_csv(url)\n",
        "print(df.head())\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBWV__enbDSy",
        "outputId": "167a1a22-2dce-45d2-ebc3-ee54f9c057fb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
            "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
            "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
            "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
            "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
            "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
            "\n",
            "   ca  thal  target  \n",
            "0   0     1       1  \n",
            "1   0     2       1  \n",
            "2   0     2       1  \n",
            "3   0     2       1  \n",
            "4   0     2       1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEjY6viQbIiG",
        "outputId": "56a69dcf-76b7-4395-f03d-670458f02fd4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"\\nClassification Report:\\n\", class_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBxQTH1vbSJS",
        "outputId": "bcfeede5-5a7b-46f6-889b-382e2fa22e1c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8688524590163934\n",
            "Precision: 0.875\n",
            "Recall: 0.875\n",
            "F1 Score: 0.875\n",
            "Confusion Matrix:\n",
            " [[25  4]\n",
            " [ 4 28]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.86      0.86        29\n",
            "           1       0.88      0.88      0.88        32\n",
            "\n",
            "    accuracy                           0.87        61\n",
            "   macro avg       0.87      0.87      0.87        61\n",
            "weighted avg       0.87      0.87      0.87        61\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***QNO2***"
      ],
      "metadata": {
        "id": "uJbdktjkbuG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***QNO3***"
      ],
      "metadata": {
        "id": "fFd-uCjzd7-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "y_train_cat = to_categorical(y_train, 10)\n",
        "y_test_cat = to_categorical(y_test, 10)\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train_cat, epochs=10, batch_size=32, validation_split=0.2)\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test_cat)\n",
        "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "y_pred_prob = model.predict(x_test)\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mW1VMLLpeENv",
        "outputId": "2be41abc-159a-4eaa-82dd-ca7303ae7ea6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7665 - loss: 0.6703 - val_accuracy: 0.8351 - val_loss: 0.4437\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.8612 - loss: 0.3870 - val_accuracy: 0.8594 - val_loss: 0.3739\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8748 - loss: 0.3337 - val_accuracy: 0.8654 - val_loss: 0.3658\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.8821 - loss: 0.3155 - val_accuracy: 0.8767 - val_loss: 0.3420\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.8873 - loss: 0.2974 - val_accuracy: 0.8811 - val_loss: 0.3356\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.8939 - loss: 0.2827 - val_accuracy: 0.8824 - val_loss: 0.3303\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.8967 - loss: 0.2714 - val_accuracy: 0.8823 - val_loss: 0.3378\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.9058 - loss: 0.2508 - val_accuracy: 0.8792 - val_loss: 0.3372\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9088 - loss: 0.2464 - val_accuracy: 0.8876 - val_loss: 0.3149\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9095 - loss: 0.2383 - val_accuracy: 0.8876 - val_loss: 0.3264\n",
            "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8853 - loss: 0.3345\n",
            "\n",
            "Test Accuracy: 0.8840\n",
            "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8539    0.8300    0.8418      1000\n",
            "           1     0.9838    0.9720    0.9779      1000\n",
            "           2     0.8166    0.7790    0.7973      1000\n",
            "           3     0.9120    0.8710    0.8910      1000\n",
            "           4     0.7585    0.8450    0.7994      1000\n",
            "           5     0.9895    0.9390    0.9636      1000\n",
            "           6     0.6986    0.7000    0.6993      1000\n",
            "           7     0.9132    0.9790    0.9450      1000\n",
            "           8     0.9607    0.9770    0.9688      1000\n",
            "           9     0.9703    0.9480    0.9590      1000\n",
            "\n",
            "    accuracy                         0.8840     10000\n",
            "   macro avg     0.8857    0.8840    0.8843     10000\n",
            "weighted avg     0.8857    0.8840    0.8843     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***QNO4***"
      ],
      "metadata": {
        "id": "jtk93pmleR-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import reuters\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "num_words = 10000\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=num_words)\n",
        "max_len = 500\n",
        "x_train = pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = pad_sequences(x_test, maxlen=max_len)\n",
        "num_classes = np.max(y_train) + 1\n",
        "y_train_cat = to_categorical(y_train, num_classes)\n",
        "y_test_cat = to_categorical(y_test, num_classes)\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=num_words, output_dim=128, input_length=max_len),\n",
        "    Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "history = model.fit(x_train, y_train_cat, epochs=5, batch_size=128, validation_split=0.2)\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test_cat)\n",
        "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
        "y_pred_prob = model.predict(x_test)\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n"
      ],
      "metadata": {
        "id": "dTKQYyBPel3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***QNO5***"
      ],
      "metadata": {
        "id": "tlq4KXVfetaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "num_classes = 100\n",
        "y_train_cat = to_categorical(y_train, num_classes)\n",
        "y_test_cat = to_categorical(y_test, num_classes)\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train_cat, epochs=10, batch_size=64, validation_split=0.2)\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test_cat)\n",
        "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
        "y_pred_prob = model.predict(x_test)\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "y_true = y_test.flatten()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0_LJOvoe_--",
        "outputId": "07aed19f-dbad-4c8b-b02c-62bbd4d47038"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "\u001b[1m169001437/169001437\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 150ms/step - accuracy: 0.0298 - loss: 4.4404 - val_accuracy: 0.1257 - val_loss: 3.8342\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 158ms/step - accuracy: 0.0993 - loss: 3.8775 - val_accuracy: 0.1856 - val_loss: 3.4915\n",
            "Epoch 3/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 176ms/step - accuracy: 0.1412 - loss: 3.6335 - val_accuracy: 0.2241 - val_loss: 3.2642\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 129ms/step - accuracy: 0.1646 - loss: 3.4563 - val_accuracy: 0.2458 - val_loss: 3.1417\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 122ms/step - accuracy: 0.1807 - loss: 3.3581 - val_accuracy: 0.2538 - val_loss: 3.0413\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 115ms/step - accuracy: 0.2021 - loss: 3.2591 - val_accuracy: 0.2705 - val_loss: 2.9616\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 116ms/step - accuracy: 0.2144 - loss: 3.1600 - val_accuracy: 0.2909 - val_loss: 2.9111\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 123ms/step - accuracy: 0.2340 - loss: 3.0678 - val_accuracy: 0.2938 - val_loss: 2.8755\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 115ms/step - accuracy: 0.2454 - loss: 3.0060 - val_accuracy: 0.3054 - val_loss: 2.7923\n",
            "Epoch 10/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 123ms/step - accuracy: 0.2584 - loss: 2.9377 - val_accuracy: 0.3162 - val_loss: 2.7490\n",
            "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 25ms/step - accuracy: 0.3223 - loss: 2.7059\n",
            "\n",
            "Test Accuracy: 0.3210\n",
            "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5455    0.6000    0.5714       100\n",
            "           1     0.4643    0.3900    0.4239       100\n",
            "           2     0.2239    0.3000    0.2564       100\n",
            "           3     0.1860    0.1600    0.1720       100\n",
            "           4     0.1235    0.1000    0.1105       100\n",
            "           5     0.3222    0.2900    0.3053       100\n",
            "           6     0.3363    0.3800    0.3568       100\n",
            "           7     0.2548    0.4000    0.3113       100\n",
            "           8     0.4286    0.4500    0.4390       100\n",
            "           9     0.4684    0.3700    0.4134       100\n",
            "          10     0.1839    0.1600    0.1711       100\n",
            "          11     0.2381    0.1500    0.1840       100\n",
            "          12     0.3478    0.2400    0.2840       100\n",
            "          13     0.1944    0.0700    0.1029       100\n",
            "          14     0.1629    0.2900    0.2086       100\n",
            "          15     0.2439    0.1000    0.1418       100\n",
            "          16     0.3366    0.3400    0.3383       100\n",
            "          17     0.5843    0.5200    0.5503       100\n",
            "          18     0.2184    0.3800    0.2774       100\n",
            "          19     0.3662    0.2600    0.3041       100\n",
            "          20     0.6667    0.6200    0.6425       100\n",
            "          21     0.3351    0.6200    0.4351       100\n",
            "          22     0.3012    0.2500    0.2732       100\n",
            "          23     0.3394    0.5600    0.4226       100\n",
            "          24     0.2576    0.6800    0.3736       100\n",
            "          25     0.3226    0.2000    0.2469       100\n",
            "          26     0.2449    0.1200    0.1611       100\n",
            "          27     0.2000    0.2900    0.2367       100\n",
            "          28     0.4173    0.5300    0.4670       100\n",
            "          29     0.3913    0.1800    0.2466       100\n",
            "          30     0.3033    0.3700    0.3333       100\n",
            "          31     0.2889    0.2600    0.2737       100\n",
            "          32     0.4074    0.2200    0.2857       100\n",
            "          33     0.2390    0.4900    0.3213       100\n",
            "          34     0.2564    0.1000    0.1439       100\n",
            "          35     0.2258    0.1400    0.1728       100\n",
            "          36     0.3864    0.3400    0.3617       100\n",
            "          37     0.2737    0.2600    0.2667       100\n",
            "          38     0.1353    0.1800    0.1545       100\n",
            "          39     0.2687    0.1800    0.2156       100\n",
            "          40     0.4390    0.1800    0.2553       100\n",
            "          41     0.5000    0.6600    0.5690       100\n",
            "          42     0.1879    0.2800    0.2249       100\n",
            "          43     0.2436    0.1900    0.2135       100\n",
            "          44     0.1558    0.1200    0.1356       100\n",
            "          45     0.1290    0.0800    0.0988       100\n",
            "          46     0.2593    0.2100    0.2320       100\n",
            "          47     0.4853    0.3300    0.3929       100\n",
            "          48     0.4932    0.7200    0.5854       100\n",
            "          49     0.4368    0.3800    0.4064       100\n",
            "          50     0.1250    0.0400    0.0606       100\n",
            "          51     0.2571    0.1800    0.2118       100\n",
            "          52     0.4777    0.7500    0.5837       100\n",
            "          53     0.5395    0.4100    0.4659       100\n",
            "          54     0.4488    0.5700    0.5022       100\n",
            "          55     0.1176    0.0200    0.0342       100\n",
            "          56     0.4019    0.4300    0.4155       100\n",
            "          57     0.4510    0.2300    0.3046       100\n",
            "          58     0.3130    0.4100    0.3550       100\n",
            "          59     0.2247    0.2000    0.2116       100\n",
            "          60     0.5342    0.7800    0.6341       100\n",
            "          61     0.3457    0.5600    0.4275       100\n",
            "          62     0.2986    0.6300    0.4051       100\n",
            "          63     0.2163    0.4500    0.2922       100\n",
            "          64     0.1333    0.1000    0.1143       100\n",
            "          65     0.0784    0.0800    0.0792       100\n",
            "          66     0.1818    0.0800    0.1111       100\n",
            "          67     0.2384    0.3600    0.2869       100\n",
            "          68     0.7093    0.6100    0.6559       100\n",
            "          69     0.4752    0.4800    0.4776       100\n",
            "          70     0.2662    0.4100    0.3228       100\n",
            "          71     0.5392    0.5500    0.5446       100\n",
            "          72     0.1481    0.0800    0.1039       100\n",
            "          73     0.3088    0.2100    0.2500       100\n",
            "          74     0.1616    0.1600    0.1608       100\n",
            "          75     0.4632    0.6300    0.5339       100\n",
            "          76     0.5204    0.5100    0.5152       100\n",
            "          77     0.0909    0.0200    0.0328       100\n",
            "          78     0.1321    0.2100    0.1622       100\n",
            "          79     0.2405    0.1900    0.2123       100\n",
            "          80     0.2353    0.0400    0.0684       100\n",
            "          81     0.3412    0.2900    0.3135       100\n",
            "          82     0.5814    0.7500    0.6550       100\n",
            "          83     0.2619    0.3300    0.2920       100\n",
            "          84     0.1169    0.0900    0.1017       100\n",
            "          85     0.4261    0.4900    0.4558       100\n",
            "          86     0.3861    0.3900    0.3881       100\n",
            "          87     0.4082    0.4000    0.4040       100\n",
            "          88     0.2558    0.2200    0.2366       100\n",
            "          89     0.3661    0.4100    0.3868       100\n",
            "          90     0.2073    0.1700    0.1868       100\n",
            "          91     0.3357    0.4700    0.3917       100\n",
            "          92     0.2931    0.1700    0.2152       100\n",
            "          93     0.2609    0.0600    0.0976       100\n",
            "          94     0.6667    0.6000    0.6316       100\n",
            "          95     0.5370    0.2900    0.3766       100\n",
            "          96     0.2109    0.2700    0.2368       100\n",
            "          97     0.1983    0.2300    0.2130       100\n",
            "          98     0.2826    0.1300    0.1781       100\n",
            "          99     0.1688    0.2700    0.2077       100\n",
            "\n",
            "    accuracy                         0.3210     10000\n",
            "   macro avg     0.3180    0.3210    0.3058     10000\n",
            "weighted avg     0.3180    0.3210    0.3058     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***QNO6***"
      ],
      "metadata": {
        "id": "zWzMkobpfQUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import reuters\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "u5HfUPsYfX0C"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = 10000\n",
        "max_len = 200\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=num_words)\n",
        "\n",
        "x_train = pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = pad_sequences(x_test, maxlen=max_len)\n",
        "\n",
        "num_classes = np.max(y_train) + 1\n",
        "y_train_cat = to_categorical(y_train, num_classes)\n",
        "y_test_cat = to_categorical(y_test, num_classes)\n",
        "\n",
        "embedding_layer = Embedding(input_dim=num_words, output_dim=128, input_length=max_len)\n",
        "model_lstm = Sequential([\n",
        "    embedding_layer,\n",
        "    LSTM(64),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoLPBJ7zfnYp",
        "outputId": "04d391d0-180f-4937-a795-bb099b8a5685"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "\u001b[1m2110848/2110848\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "print(\"\\nğŸ”· Training LSTM Model:\")\n",
        "model_lstm.fit(x_train, y_train_cat, epochs=5, batch_size=128, validation_split=0.2)\n",
        "loss_lstm, acc_lstm = model_lstm.evaluate(x_test, y_test_cat)\n",
        "print(f\"\\nLSTM Test Accuracy: {acc_lstm:.4f}\")\n",
        "\n",
        "y_pred_lstm = np.argmax(model_lstm.predict(x_test), axis=1)\n",
        "print(\"\\nLSTM Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_lstm, digits=4))\n",
        "model_gru = Sequential([\n",
        "    embedding_layer,\n",
        "    GRU(64),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDHX697kfuCx",
        "outputId": "f4203c08-d8c8-40ad-8c56-92fb271d671a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ”· Training LSTM Model:\n",
            "Epoch 1/5\n",
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 480ms/step - accuracy: 0.3132 - loss: 3.2714 - val_accuracy: 0.3795 - val_loss: 2.3266\n",
            "Epoch 2/5\n",
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 475ms/step - accuracy: 0.4606 - loss: 2.1668 - val_accuracy: 0.3450 - val_loss: 2.7793\n",
            "Epoch 3/5\n",
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 476ms/step - accuracy: 0.3422 - loss: 2.2958 - val_accuracy: 0.5186 - val_loss: 1.8261\n",
            "Epoch 4/5\n",
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 503ms/step - accuracy: 0.5421 - loss: 1.7098 - val_accuracy: 0.5426 - val_loss: 1.7389\n",
            "Epoch 5/5\n",
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 508ms/step - accuracy: 0.5831 - loss: 1.6028 - val_accuracy: 0.5654 - val_loss: 1.6829\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5717 - loss: 1.7086\n",
            "\n",
            "LSTM Test Accuracy: 0.5623\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 56ms/step\n",
            "\n",
            "LSTM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000        12\n",
            "           1     0.1433    0.4000    0.2111       105\n",
            "           2     0.0000    0.0000    0.0000        20\n",
            "           3     0.9436    0.8844    0.9130       813\n",
            "           4     0.7373    0.8291    0.7805       474\n",
            "           5     0.0000    0.0000    0.0000         5\n",
            "           6     0.0000    0.0000    0.0000        14\n",
            "           7     0.0000    0.0000    0.0000         3\n",
            "           8     0.0000    0.0000    0.0000        38\n",
            "           9     0.0000    0.0000    0.0000        25\n",
            "          10     0.0827    0.3667    0.1350        30\n",
            "          11     0.1250    0.0723    0.0916        83\n",
            "          12     0.0000    0.0000    0.0000        13\n",
            "          13     0.0000    0.0000    0.0000        37\n",
            "          14     0.0000    0.0000    0.0000         2\n",
            "          15     0.0000    0.0000    0.0000         9\n",
            "          16     0.0000    0.0000    0.0000        99\n",
            "          17     0.0000    0.0000    0.0000        12\n",
            "          18     0.0000    0.0000    0.0000        20\n",
            "          19     0.1937    0.6917    0.3026       133\n",
            "          20     0.0000    0.0000    0.0000        70\n",
            "          21     0.0000    0.0000    0.0000        27\n",
            "          22     0.0000    0.0000    0.0000         7\n",
            "          23     0.0000    0.0000    0.0000        12\n",
            "          24     0.0000    0.0000    0.0000        19\n",
            "          25     0.0000    0.0000    0.0000        31\n",
            "          26     0.0000    0.0000    0.0000         8\n",
            "          27     0.0000    0.0000    0.0000         4\n",
            "          28     0.0000    0.0000    0.0000        10\n",
            "          29     0.0000    0.0000    0.0000         4\n",
            "          30     0.0000    0.0000    0.0000        12\n",
            "          31     0.0000    0.0000    0.0000        13\n",
            "          32     0.0000    0.0000    0.0000        10\n",
            "          33     0.0000    0.0000    0.0000         5\n",
            "          34     0.0000    0.0000    0.0000         7\n",
            "          35     0.0000    0.0000    0.0000         6\n",
            "          36     0.0000    0.0000    0.0000        11\n",
            "          37     0.0000    0.0000    0.0000         2\n",
            "          38     0.0000    0.0000    0.0000         3\n",
            "          39     0.0000    0.0000    0.0000         5\n",
            "          40     0.0000    0.0000    0.0000        10\n",
            "          41     0.0000    0.0000    0.0000         8\n",
            "          42     0.0000    0.0000    0.0000         3\n",
            "          43     0.0000    0.0000    0.0000         6\n",
            "          44     0.0000    0.0000    0.0000         5\n",
            "          45     0.0000    0.0000    0.0000         1\n",
            "\n",
            "    accuracy                         0.5623      2246\n",
            "   macro avg     0.0484    0.0705    0.0529      2246\n",
            "weighted avg     0.5211    0.5623    0.5282      2246\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_gru.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "print(\"\\nğŸ”· Training GRU Model:\")\n",
        "model_gru.fit(x_train, y_train_cat, epochs=5, batch_size=128, validation_split=0.2)\n",
        "loss_gru, acc_gru = model_gru.evaluate(x_test, y_test_cat)\n",
        "print(f\"\\nGRU Test Accuracy: {acc_gru:.4f}\")\n",
        "\n",
        "y_pred_gru = np.argmax(model_gru.predict(x_test), axis=1)\n",
        "print(\"\\nGRU Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_gru, digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DiRwWOcfw_X",
        "outputId": "0358ae34-cae0-42c0-a991-67b41943bff2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ”· Training GRU Model:\n",
            "Epoch 1/5\n",
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 529ms/step - accuracy: 0.3307 - loss: 3.0793 - val_accuracy: 0.3801 - val_loss: 1.9975\n",
            "Epoch 2/5\n",
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 527ms/step - accuracy: 0.4711 - loss: 1.8292 - val_accuracy: 0.5392 - val_loss: 1.7762\n",
            "Epoch 3/5\n",
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 554ms/step - accuracy: 0.5903 - loss: 1.5295 - val_accuracy: 0.5543 - val_loss: 1.6885\n",
            "Epoch 4/5\n",
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 552ms/step - accuracy: 0.6107 - loss: 1.4517 - val_accuracy: 0.5526 - val_loss: 1.6963\n",
            "Epoch 5/5\n",
            "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 545ms/step - accuracy: 0.6353 - loss: 1.3794 - val_accuracy: 0.5676 - val_loss: 1.6948\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - accuracy: 0.5956 - loss: 1.6867\n",
            "\n",
            "GRU Test Accuracy: 0.5793\n",
            "\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step\n",
            "\n",
            "GRU Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000        12\n",
            "           1     0.1787    0.5429    0.2689       105\n",
            "           2     0.0000    0.0000    0.0000        20\n",
            "           3     0.9501    0.9139    0.9317       813\n",
            "           4     0.7140    0.8165    0.7618       474\n",
            "           5     0.0000    0.0000    0.0000         5\n",
            "           6     0.0000    0.0000    0.0000        14\n",
            "           7     0.0000    0.0000    0.0000         3\n",
            "           8     0.0000    0.0000    0.0000        38\n",
            "           9     0.0000    0.0000    0.0000        25\n",
            "          10     0.1443    0.4667    0.2205        30\n",
            "          11     0.1053    0.1687    0.1296        83\n",
            "          12     0.0000    0.0000    0.0000        13\n",
            "          13     0.0000    0.0000    0.0000        37\n",
            "          14     0.0000    0.0000    0.0000         2\n",
            "          15     0.0000    0.0000    0.0000         9\n",
            "          16     0.2500    0.1818    0.2105        99\n",
            "          17     0.0000    0.0000    0.0000        12\n",
            "          18     0.0000    0.0000    0.0000        20\n",
            "          19     0.2259    0.5113    0.3134       133\n",
            "          20     0.0000    0.0000    0.0000        70\n",
            "          21     0.0000    0.0000    0.0000        27\n",
            "          22     0.0000    0.0000    0.0000         7\n",
            "          23     0.0000    0.0000    0.0000        12\n",
            "          24     0.0000    0.0000    0.0000        19\n",
            "          25     0.0000    0.0000    0.0000        31\n",
            "          26     0.0000    0.0000    0.0000         8\n",
            "          27     0.0000    0.0000    0.0000         4\n",
            "          28     0.0000    0.0000    0.0000        10\n",
            "          29     0.0000    0.0000    0.0000         4\n",
            "          30     0.0000    0.0000    0.0000        12\n",
            "          31     0.0000    0.0000    0.0000        13\n",
            "          32     0.0000    0.0000    0.0000        10\n",
            "          33     0.0000    0.0000    0.0000         5\n",
            "          34     0.0000    0.0000    0.0000         7\n",
            "          35     0.0000    0.0000    0.0000         6\n",
            "          36     0.0000    0.0000    0.0000        11\n",
            "          37     0.0000    0.0000    0.0000         2\n",
            "          38     0.0000    0.0000    0.0000         3\n",
            "          39     0.0000    0.0000    0.0000         5\n",
            "          40     0.0000    0.0000    0.0000        10\n",
            "          41     0.0000    0.0000    0.0000         8\n",
            "          42     0.0000    0.0000    0.0000         3\n",
            "          43     0.0000    0.0000    0.0000         6\n",
            "          44     0.0000    0.0000    0.0000         5\n",
            "          45     0.0000    0.0000    0.0000         1\n",
            "\n",
            "    accuracy                         0.5793      2246\n",
            "   macro avg     0.0558    0.0783    0.0617      2246\n",
            "weighted avg     0.5332    0.5793    0.5462      2246\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}